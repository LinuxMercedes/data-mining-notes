\documentclass{article}

\begin{document}

\section{Definitions}

\section{Algorithms}

\subsection{Support Vector Machines}
\begin{itemize}
	\item Assume that decision attribute is binary
	\item Hyperplane: plane that separates positive and negative instances in dataset
	\item Support vectors: data points (actual instances) that define the hyperplane and maximizes the distance between the hyperplane and support vectors
	\item Example:
		\begin{itemize}
			\item Let support vectors be $s_1$, $s_2$, $s_3$ and the decision attribute for each be $d_1$, $d_2$, $d_3$
			\item $\alpha_1 \phi(s_1) \bullet \phi(s_1) + \alpha_2 \phi(s_2) \bullet \phi(s_1) + \alpha_3 \phi(s_3) \bullet \phi(s_1) = d_1$
			\item $\alpha_1 \phi(s_1) \bullet \phi(s_2) + \alpha_2 \phi(s_2) \bullet \phi(s_2) + \alpha_3 \phi(s_3) \bullet \phi(s_2) = d_1$
			\item $\alpha_1 \phi(s_1) \bullet \phi(s_1) + \alpha_2 \phi(s_2) \bullet \phi(s_3) + \alpha_3 \phi(s_3) \bullet \phi(s_3) = d_1$
			\item Solve for $\alpha_1$, $\alpha_2$, $\alpha_3$
			\item Hyperplane is $w' = \alpha_1 s_1 + \alpha_2 s_2 + \alpha_3 s_3$
			\item $\phi$ is the identity function for a linear SVM (i.e.\ the hyperplane is a linear plane/line)
		\end{itemize}
	\item $\phi$
		\begin {itemize}
		  \item TODO: read the book and pull more clear info from it for this section
			\item Allows mapping from nonlinear ``input space'' to linear ``feature space'' where we perform the classification
			\item Find a ``kernel function'' to transform (possibly higher dimesionality data) into a space such that it is better structured (and thus more easily classified)
			\item ``Kernel Trick'': use a kernel function instead of bullet product between vectors
			\item Example kernel functions:
				\begin{itemize}
					\item Polynomial Learning Machine: $K(x, y) = (x \bullet y + 1) ^ p$
					\item Radial-Basis Function (or Gaussian) Network: $K(x, y) = \frac {exp\{-||x - y||^2\}} {2\sigma^2}$
					\item Sigmoid (or Neural Net Activation) Network: $K(x, y) = tanh(\kappa x \bullet y - \delta)$
				\end{itemize}
		\end{itemize}
	\item Classifying new instances:
		\begin{itemize}
			\item Let $v$ be the new vector to classify
			\item Let $n$ = \# support vectors
			\item Let \begin{displaymath}
					\sigma(x) = \left\{
					\begin{array}{lr}
						positive & : x >= 0 \\
						negative & : x < 0
					\end{array}
					\right.
				\end{displaymath}
			\item Compute $f(v) = \sigma(\sum_{i=1}^{n} \alpha_i \phi(s_i) \bullet \phi(v) )$
		\end{itemize}
\end{itemize}

\subsection{Bayesian Networks}
\begin{itemize}
	\item Probabilistic model representing random variables and the relationships between them in a DAG.
	\item Disconnected nodes are independent
	\item Each node is a probability function that takes as inputs the values of its parent nodes
	\item Node interpreted as: What is the probability of this node being true, given that parent1, parent2, \ldots have particular values?
	\item $P(G = T | R = T) = \frac{P(G = T) P(R = T)}{P(G = T)}$
	\item Bayes' Rule: $P(H | E) = \frac{P(E | H) P(H)}{P(E)}$
	\item TODO: Laplace Estimation
	\item TODO: Probability Calculation
	\item Categorizing a new instance:
		\begin{itemize}
			\item Calculate probability for each node based on instance values
			\item Calculate probability of each possible decision attribute value based on node values
			\item Normalize resulting probabilities ($P(this) / \sum P(each\ value)$)
		\end{itemize}
	\item Prior Knowledge Algorithm: Adding edges to Bayesian Networks
		\begin{itemize}
			\item Let $T = (X_1, X_2, \cdots, X_n)$ be an ordering of attributes
			\item For $j = 1 to n$:
			\item Let $S = T[1 to j - 1]$ (the attributes that precede $T[j]$)
			\item Remove attributes from $S$ that do not affect $T[j]$ based on prior/domain-specific Knowledge
			\item Create an edge from every element in $S$ to $T[j]$
		\end{itemize}
	\item K2: Adding edges to Bayesian Networks
		\begin{itemize}
			\item Process each node in order and greedily considers adding edges from previously processed nodes to current node.
			\item In each step, add the edges that maximize the node's score, optionally limiting to a set number of parents
			\item Dependent on initial ordering of nodes, so running on different orderings is recommended
			\item Score for node $i$:
				\begin{itemize}
					\item Let $V_i$ be the possible values for node $i$
					\item Let $\pi_i$ be the parents of node $i$
					\item Let $\theta_i$ be all attribute-value pairs in $\pi_i$
					\item Let $\alpha_{ijk}$ be the number of instances where attribute $x_i$ has value $V_i[k]$ and the attributes in $\pi_i$ have the values in $\theta_i[j]$
					\item Let $N_{ij}$ be the sum of the number of instances that have values matching an entry in $\theta_i$
					\item If $\theta_i = \emptyset$, $N_{ij} = \sum_{k=1}^{|V_i|} \alpha_{ijk}$
					\item $f(i, \pi_i) = \prod_{j=1}^{|\theta_i|} 	\frac{(|V_i| - 1)!}{N_{ij} + |V_i| - 1)!} \prod_{k=1}^{|V_i|} \alpha_{ijk}! $
				\end{itemize}
			\item K2 Variations:
				\begin{itemize}
					\item Markov Blanket: Make a supernode containing node x (the decision node), node x's parents, and node x's children, and their parents.
					\item Start with a given network and consider adding, deleting, or switching directions of edges between arbitrary nodes
				\end{itemize}
		\end{itemize}
	\item Scoring Bayesian Networks:
		\begin{itemize}
			\item Maximum Likelyhood Criterion (bigger is better): $\sum_{i=1}^{attributes}\sum_{j=1}^{|\theta_i|}\sum_{k=1}^{|V_i|} \alpha_{ijk} log \frac{\alpha_{ijk}}{N_ {ij}}$
			\item Akaike Information Criterion (smaller is better): $MLC - #paramenters$
			\item Bayesion Information Criterion (bigger is better): $MLC - 0.5 * log_2(#instances) * #parameters$
			\item Minimum Description Length (smaller is better): $-BIC$
		\end{itemize}
\end{itemize}

\end{document}

